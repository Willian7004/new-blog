## 对LLM的两个级别的需求

对LLM的需求中，几个月能实现的是软件自由以及可靠的computer use功能。有报道说Gemini2.5 pro exp能实现根据视频使用html创建类似的效果，算是验证了一些算法竞赛选手提到的codeforces 2400分以上需要泛化能力的观点。不过为了进一步实现更多编程需求，可能需要采用混合注意力、集束搜索、优化的强化学习方法和更长的思维链的第三代推理模型。

最近看到有人提及拉马努金、伽罗瓦等数学天才的事迹，有人提到这些天才将是LLM难以企及的。受后续推理scaling的瓶颈以及难以强化学习数据难以制作、验证难度大等问题，如果最终无法实现自我迭代，后续的LLM完成Frontier Math等测试集的难度仍然较大。不少科技是由顶尖研究者推动的，如果LLM无法完成Frontier Math等测试集，在科研中仍然只能用于辅助而不是独立完成新的科学发现。

更新：Epoch AI发布了研究报告，认为推理模型会在大约一年后到达瓶颈，主要受强化学习成本和数据限制。这与我之前的推理scaling只有3到4代的判断区别不大。这意味着在没有新的scaling且成本可控的情况下，数年内很可能不会出现AGI以实现上文的高级别需求。