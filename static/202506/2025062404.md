## DeepWiki深入研究：MoE模型推理速度问题的可能原因

DeepWiki是由Devin推出的分析GitHub仓库的应用，为不少公开仓库创建了索引并且可以就仓库相关问题进行免费提问，这里主要用来分析一些长期未解决的问题。

我使用基于llama.cpp的一些推理应用时遇到不同moe模型速度优化不一致的问题，考虑到当前wiki时间较旧，提问中未包含近期遇到的minicpm4的类似问题。

7.2更新：最近测试了Ring Lite也能实现明显提速，在双路ddr4下达到16.75token/s，同激活参数量下比qwen系列快不少。

#### user:

qwen2 57b和qwen3 30b为moe模型，推理速度明显高于同参数量dense模型。但使用deepseek v2 16b和ganite3 3b等moe模型的推理速度与同参数量dense模型区别不大。造成这一问题的原因可能有哪些？

#### assistant:

基于对llama.cpp代码库的分析，造成不同MoE模型推理速度差异的主要原因包括以下几个方面：

#### 1. Expert配置和架构差异

不同MoE模型的expert配置存在显著差异。Qwen2MoE和DeepSeek使用了不同的expert数量和expert使用策略： [1](#0-0) [2](#0-1) 

Qwen2MoE和Qwen3MoE支持shared expert分支，这可以提高计算效率，而DeepSeek模型在某些层采用dense前导层的混合架构。

#### 2. Expert Gating Function的差异

不同模型使用不同的expert gating函数，这直接影响expert selection的计算开销： [3](#0-2) [4](#0-3) 

Softmax和Sigmoid的计算复杂度不同，会影响整体推理速度。

#### 3. Expert Selection和Routing的实现

MoE的核心性能瓶颈在于expert selection和routing操作，特别是`mul_mat_id`操作： [5](#0-4) [6](#0-5) 

这个操作需要根据selected experts进行稀疏矩阵乘法，其效率很大程度上取决于expert选择的模式和硬件优化。

#### 4. Expert权重归一化和缩放

不同模型采用不同的权重处理策略： [7](#0-6) [8](#0-7) 

权重归一化和缩放操作会增加额外的计算开销。

#### 5. Expert聚合策略

不同的expert结果聚合方式也会影响性能： [9](#0-8) 

#### 6. 模型架构特殊性

DeepSeek模型具有特殊的混合架构，包含dense前导层和MoE层的组合： [10](#0-9) 

这种混合架构可能导致无法充分利用MoE的稀疏性优势。

#### Notes

造成MoE模型推理速度差异的主要原因是多方面的：

1. **Expert配置差异**：不同模型的expert数量、使用数量和FFN大小配置不同
2. **Gating function实现**：Softmax vs Sigmoid的计算开销差异
3. **Expert selection效率**：稀疏矩阵乘法的硬件优化程度
4. **架构设计**：是否包含shared experts和dense层的混合使用
5. **权重处理开销**：归一化和缩放操作的额外计算成本
6. **内存访问模式**：不同expert selection模式对GPU内存访问效率的影响

Qwen2/3 MoE模型可能在这些方面进行了更好的优化，而DeepSeek V2和Granite3等模型的某些设计选择可能限制了MoE的稀疏性优势，导致其推理速度与dense模型相近。
